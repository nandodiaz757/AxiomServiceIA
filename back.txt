from fastapi import FastAPI, Query, BackgroundTasks, Request, APIRouter, HTTPException, Depends
from typing import Optional, Union, List, Dict, Any
from pydantic import BaseModel, Field
import sqlite3, json, joblib, numpy as np, os, hashlib, logging, asyncio
from sklearn.cluster import MiniBatchKMeans
from hmmlearn import hmm
from fastapi.responses import JSONResponse
from fastapi_utils.tasks import repeat_every
from diff_model.predict_diff import router as diff_router
from datetime import datetime

# =========================================================
# CONFIGURACI√ìN
# =========================================================
app = FastAPI()
router = APIRouter() 
DB_NAME = "accessibility.db"
MODELS_DIR = "models"
logger = logging.getLogger("myapp")
logger.setLevel(logging.INFO)
app.include_router(diff_router)



if not logger.hasHandlers():
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

os.makedirs(MODELS_DIR, exist_ok=True)
TRAIN_GENERAL_ON_COLLECT = True

# Locks para entrenamientos concurrentes
_model_locks: Dict[str, asyncio.Lock] = {}


# === Reentrenamiento autom√°tico del modelo diff ===
@app.on_event("startup")
@repeat_every(seconds=3600)  # cada hora, ajusta el intervalo a lo que necesites
def retrain_model() -> None:
    """
    Reentrena el modelo diff con las √∫ltimas aprobaciones/rechazos
    sin necesidad de parar el servidor.
    """
    from diff_model.train_diff_model import train_and_save
    train_and_save()


def _get_lock(key: str) -> asyncio.Lock:
    if key not in _model_locks:
        _model_locks[key] = asyncio.Lock()
    return _model_locks[key]

# =========================================================
# BASE DE DATOS
# =========================================================

def get_db():
    """
    Devuelve una conexi√≥n SQLite que se cierra autom√°ticamente
    al finalizar la petici√≥n.
    """
    db = sqlite3.connect(DB_NAME)
    db.row_factory = sqlite3.Row  # Para poder acceder a columnas por nombre
    try:
        yield db
    finally:
        db.close()
        
        
def init_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS accessibility_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            timestamp INTEGER,
            event_type TEXT,
            event_type_name TEXT,
            package_name TEXT,
            class_name TEXT,
            text TEXT,
            content_description TEXT,
            screens_id TEXT,
            screen_names TEXT,
            header_text TEXT,
            actual_device TEXT,
            signature TEXT,
            version TEXT,
            collect_node_tree TEXT,
            additional_info TEXT,
            tree_data TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS screen_diffs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            screen_name TEXT,
            removed TEXT,
            added TEXT,
            modified TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_trace (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            screen_name TEXT,
            message TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_approvals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            diff_id INTEGER,
            approved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_rejections (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            diff_id INTEGER,
            rejected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE INDEX IF NOT EXISTS idx_data_tester_build_screen
        ON accessibility_data(tester_id, build_id, screen_names, created_at DESC)
    """)
    c.execute("""
        CREATE INDEX IF NOT EXISTS idx_diffs_tester_build_screen
        ON screen_diffs(tester_id, build_id, screen_name, created_at DESC)
    """)
    conn.commit()
    conn.close()
    
init_db()

# =========================================================
# MODELOS DE ENTRADA
# =========================================================
class AccessibilityEvent(BaseModel):
    # Identificaci√≥n de tester y build (usa alias en camelCase para que coincida con el JSON entrante)
    tester_id: Optional[str] = Field(None, alias="actualDevice")
    build_id: Optional[str] = Field(None, alias="version")
    
    # Datos b√°sicos del evento de accesibilidad
    timestamp: Optional[int] = Field(None, alias="timestamp")
    event_type: Optional[int] = Field(None, alias="eventType")
    event_type_name: Optional[str] = Field(None, alias="eventTypeName")
    package_name: Optional[str] = Field(None, alias="packageName")
    class_name: Optional[str] = Field(None, alias="className")
    text: Optional[str] = Field(None, alias="text")
    content_description: Optional[str] = Field(None, alias="contentDescription")
    
    # Informaci√≥n de flujo/pantalla
    screens_id: Optional[str] = Field(None, alias="screensId")
    screen_names: Optional[str] = Field(None, alias="screenNames")
    header_text: Optional[str] = Field(None, alias="headerText")
    
    # Datos de dispositivo y versi√≥n de la app
    actual_device: Optional[str] = Field(None, alias="actualDevice")    
    version: Optional[str] = Field(None, alias="version")
    
    # √Årbol de nodos capturado (puede ser dict o lista de nodos)
    # collect_node_tree: Optional[Union[Dict, List]] = Field(None, alias="collectNodeTree")
    collect_node_tree: Optional[Union[Dict[str, Any], List[Any]]] = Field(
        None, alias="collectNodeTree"
    )
    
    # Datos adicionales para enriquecer el modelo (libres)
    additional_info: Optional[Dict[str, Any]] = Field(None, alias="additionalInfo")
    tree_data: Optional[Dict[str, Any]] = Field(None, alias="treeData")

    class Config:
    # Permite poblar el modelo con los nombres de campo internos o los alias del JSON
        allow_population_by_field_name = True
    
    # Acepta campos extra que la app pueda enviar en el futuro sin romper validaci√≥n
        extra = "allow"

# =========================================================
# UTILIDADES PARA √ÅRBOLES Y HASH ESTABLE
# =========================================================
SAFE_KEYS = ["className", "text", "desc", "viewId", "pkg"]


def ui_structure_features(tree: dict) -> list[float]:
    """
    Cuenta componentes y propiedades de accesibilidad.
    Devuelve:
    [buttons, text_fields, menus, recycler_views, web_views,
     enabled_count, clickable_count, focusable_count]
    """
    counts = {
        "buttons": 0,
        "text_fields": 0,
        "menus": 0,
        "recycler_views": 0,
        "web_views": 0
    }
    props = {
        "enabled": 0,
        "clickable": 0,
        "focusable": 0
    }

    def traverse(node):
        if not isinstance(node, dict):
            return
        cls = node.get("className", "")
        if "Button" in cls:        counts["buttons"]        += 1
        if "EditText" in cls:      counts["text_fields"]    += 1
        if "Menu" in cls:          counts["menus"]          += 1
        if "RecyclerView" in cls:  counts["recycler_views"] += 1
        if "WebView" in cls:       counts["web_views"]      += 1

        if node.get("enabled"):    props["enabled"]   += 1
        if node.get("clickable"):  props["clickable"] += 1
        if node.get("focusable"):  props["focusable"] += 1

        for ch in node.get("children", []):
            traverse(ch)

    traverse(tree)
    return list(counts.values()) + list(props.values())

def input_features(events):
    total_chars   = sum(len(e.text or "") for e in events if e.type=="input")
    upper_ratio   = total_chars and sum(ch.isupper() for e in events if e.type=="input" for ch in e.text)/total_chars
    action_seq    = [e.type for e in events]   # p.ej. ["tap","scroll","tap"]
    seq_entropy   = sequence_entropy(action_seq)
    return [total_chars, upper_ratio, seq_entropy]

def ensure_list(tree):
    if isinstance(tree, str):
        try:
            return json.loads(tree)
        except Exception:
            return []
    return tree or []

def normalize_node(node: Dict) -> Dict:
    return {k: (node.get(k) or "") for k in SAFE_KEYS}

def normalize_tree(nodes: List[Dict]) -> List[Dict]:
    return sorted([normalize_node(n) for n in nodes if isinstance(n, dict)],
                  key=lambda n: (n["className"], n["text"]))

def stable_signature(nodes: List[Dict]) -> str:
    return hashlib.sha256(json.dumps(normalize_tree(nodes), sort_keys=True).encode()).hexdigest()


def compare_trees(old_tree, new_tree):
    old_tree = ensure_list(old_tree)
    new_tree = ensure_list(new_tree)
    def make_key(n):
        if not isinstance(n, dict): return None
        parts = [n.get("viewId"), n.get("className"), n.get("headerText")]
        return "|".join([str(p) for p in parts if p])
    old_idx = {make_key(n): n for n in old_tree if make_key(n)}
    new_idx = {make_key(n): n for n in new_tree if make_key(n)}
    removed = [n for k, n in old_idx.items() if k not in new_idx]
    added   = [n for k, n in new_idx.items() if k not in old_idx]
    modified = []
    for k, nn in new_idx.items():
        if k in old_idx:
            changes = {f: {"old": old_idx[k].get(f), "new": nn.get(f)}
                       for f in ["text","contentDescription","checked","enabled"]
                       if old_idx[k].get(f) != nn.get(f)}
            if changes:
                modified.append({"node": {"key": k}, "changes": changes})
    return removed, added, modified

# =========================================================
# VECTORIZACI√ìN Y FEATURES
# =========================================================
def compare_trees(old_tree, new_tree):
    old_tree = ensure_list(old_tree)
    new_tree = ensure_list(new_tree)
    def make_key(n):
        if not isinstance(n, dict): return None
        parts = [n.get("viewId"), n.get("className"), n.get("headerText")]
        return "|".join([str(p) for p in parts if p])
    old_idx = {make_key(n): n for n in old_tree if make_key(n)}
    new_idx = {make_key(n): n for n in new_tree if make_key(n)}
    removed = [n for k, n in old_idx.items() if k not in new_idx]
    added   = [n for k, n in new_idx.items() if k not in old_idx]
    modified = []
    for k, nn in new_idx.items():
        if k in old_idx:
            changes = {f: {"old": old_idx[k].get(f), "new": nn.get(f)}
                       for f in ["text","contentDescription","checked","enabled"]
                       if old_idx[k].get(f) != nn.get(f)}
            if changes:
                modified.append({"node": {"key": k}, "changes": changes})
    return removed, added, modified

def features_from_rows(rows) -> np.ndarray:
    vecs = [vector_from_tree(r[0]) for r in rows if r and r[0]]
    return np.vstack(vecs) if vecs else np.empty((0,3))
    
    
# =========================================================
# VECTORIZACI√ìN Y FEATURES
# =========================================================
def vector_from_tree(tree_str: str) -> np.ndarray:
    """
    Genera un vector:
    [total_nodes, max_depth, text_nodes,
     buttons, text_fields, menus, recycler_views, web_views,
     enabled_count, clickable_count, focusable_count]
    """
    try:
        tree = json.loads(tree_str)
    except Exception:
        # Vector con 11 ceros si el JSON no es v√°lido
        return np.zeros(11, dtype=float)

    def walk(node, depth=1):
        if not isinstance(node, dict):
            return (0, depth, 0)
        children = node.get("children", [])
        total, max_d, txt = 1, depth, 1 if node.get("text") else 0
        for ch in children:
            t, d, c = walk(ch, depth + 1)
            total += t
            max_d = max(max_d, d)
            txt += c
        return total, max_d, txt

    if isinstance(tree, list):
        totals = [walk(n) for n in tree]
        total_nodes = sum(t[0] for t in totals)
        max_depth   = max((t[1] for t in totals), default=0)
        text_nodes  = sum(t[2] for t in totals)
        # Para ui_structure_features, empaquetamos en un dict "ra√≠z"
        struct_vec  = ui_structure_features({"children": tree})
    else:
        total_nodes, max_depth, text_nodes = walk(tree)
        struct_vec  = ui_structure_features(tree)

    base_vec = [total_nodes, max_depth, text_nodes]
    return np.array(base_vec + struct_vec, dtype=float)

def features_from_rows(rows) -> np.ndarray:
    vectors = [vector_from_tree(r[0]) for r in rows if r and r[0]]
    return np.vstack(vectors) if vectors else np.empty((0, 11))    

# =========================================================
# ENTRENAMIENTO H√çBRIDO (KMeans + HMM)
# =========================================================
# async def _train_model_hybrid(X, tester_id, build_id, lock: asyncio.Lock,
                              # max_clusters=3, min_samples=2,
                              # desc="", n_hmm_states=5):
    # async with lock:
        # if len(X) < min_samples:
            # logger.info(f"[train_hybrid] muestras insuficientes {desc}")
            # return
        # kmeans = MiniBatchKMeans(n_clusters=min(max_clusters,len(X)), random_state=42).fit(X)
        # hmm_model = hmm.GaussianHMM(
            # n_components=min(n_hmm_states,len(X)),
            # covariance_type="diag", n_iter=100
        # ).fit(X, [len(X)])
        # base = os.path.join(MODELS_DIR, tester_id or "general", build_id or "default")
        # os.makedirs(base, exist_ok=True)
        # joblib.dump({"kmeans": kmeans, "hmm": hmm_model}, os.path.join(base, "model.pkl"))
        # logger.info(f"[train_hybrid] Modelo guardado {desc}")
        
        
async def _train_model_hybrid(
    X,
    tester_id,
    build_id,
    lock: asyncio.Lock,
    max_clusters=2,        # ‚Üì menos clusters en KMeans
    min_samples=2,
    desc="",
    n_hmm_states=2         # ‚Üì menos estados ocultos en el HMM
):
    async with lock:
        if len(X) < min_samples:
            logger.info(f"[train_hybrid] muestras insuficientes {desc}")
            return

        # KMeans con m√°ximo 2 clusters (o tantos como muestras haya)
        kmeans = MiniBatchKMeans(
            n_clusters=min(max_clusters, len(X)),
            random_state=42
        ).fit(X)

        # HMM con m√°ximo 2 estados ocultos
        hmm_model = hmm.GaussianHMM(
            n_components=min(n_hmm_states, len(X)),
            covariance_type="diag",
            n_iter=100    # puedes bajar a 50 si quieres a√∫n m√°s r√°pido
        ).fit(X, [len(X)])

        base = os.path.join(MODELS_DIR, tester_id or "general", build_id or "default")
        os.makedirs(base, exist_ok=True)
        joblib.dump({"kmeans": kmeans, "hmm": hmm_model}, os.path.join(base, "model.pkl"))
        logger.info(f"[train_hybrid] Modelo guardado {desc}")

# =========================================================
# ENTRENAMIENTO H√çBRIDO (KMeans + HMM)  ‚Äì versi√≥n mejorada
# =========================================================


async def analyze_and_train(event: AccessibilityEvent):
    norm = _normalize_event_fields(event)
    t_id, b_id = norm.get("tester_id_norm"), norm.get("build_id_norm")
    s_name = event.screen_names or ""
    latest_tree = ensure_list(event.collect_node_tree or event.tree_data or [])
    sig = stable_signature(latest_tree)

    with sqlite3.connect(DB_NAME) as conn:
        if conn.execute("""
            SELECT 1 FROM accessibility_data
            WHERE IFNULL(screen_names,'')=IFNULL(?, '')
              AND IFNULL(tester_id,'')=IFNULL(?, '')
              AND signature=?
            LIMIT 1
        """, (s_name, t_id, sig)).fetchone():
            logger.info("[diff-trace] firma ya existente, skip")
            return

    with sqlite3.connect(DB_NAME) as conn:
        prev = conn.execute("""
            SELECT collect_node_tree
            FROM accessibility_data
            WHERE IFNULL(screen_names,'')=IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT 3
        """, (s_name,)).fetchall()

    prev_trees = [ensure_list(r[0]) for r in prev]
    if not prev_trees:
        _insert_diff_trace(t_id, b_id, s_name, "Sin cambios (primera captura)")
        await _train_incremental_logic_hybrid(t_id, b_id)
        if TRAIN_GENERAL_ON_COLLECT:
            await _train_general_logic_hybrid()
        return

    removed, added, modified = [], [], []
    for p in prev_trees:
        r,a,m = compare_trees(p, latest_tree)
        removed += r; added += a; modified += m

    if not (removed or added or modified):
        _insert_diff_trace(t_id, b_id, s_name, "Sin cambios significativos")
        return

    removed_j, added_j, mod_j = map(lambda x: json.dumps(x, sort_keys=True),
                                    (removed, added, modified))
    with sqlite3.connect(DB_NAME) as conn:
        if conn.execute("""
            SELECT 1 FROM screen_diffs
            WHERE IFNULL(screen_name,'')=IFNULL(?, '')
              AND removed=? AND added=? AND modified=?
            LIMIT 1
        """, (s_name, removed_j, added_j, mod_j)).fetchone():
            return
        if conn.execute("""
            SELECT 1
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.screen_name=? AND s.removed=? AND s.added=? AND s.modified=?
            LIMIT 1
        """, (s_name, removed_j, added_j, mod_j)).fetchone():
            return
        conn.execute("""
            INSERT INTO screen_diffs (tester_id, build_id, screen_name, removed, added, modified)
            VALUES (?,?,?,?,?,?)
        """, (t_id, b_id, s_name, removed_j, added_j, mod_j))
        conn.commit()

    _insert_diff_trace(t_id, b_id, s_name,
                       f"Removed={len(removed)}, Added={len(added)}, Modified={len(modified)}")

    await _train_incremental_logic_hybrid(t_id, b_id)
    if TRAIN_GENERAL_ON_COLLECT:
        await _train_general_logic_hybrid()


# =========================================================
# ENTRENAMIENTO INCREMENTAL (versi√≥n original conservada)
# =========================================================
async def _train_incremental_logic_hybrid(tester_id: str, build_id: str,
                                          batch_size=200, min_samples=2):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            SELECT DISTINCT collect_node_tree
            FROM accessibility_data
            WHERE collect_node_tree IS NOT NULL
              AND IFNULL(tester_id,'')=IFNULL(?, '')
              AND IFNULL(build_id,'')=IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT ?
        """, (tester_id, build_id, batch_size))
        rows = c.fetchall()
        if len(rows) < min_samples:
            need = min_samples - len(rows)
            c.execute("""
                SELECT DISTINCT collect_node_tree
                FROM accessibility_data
                WHERE collect_node_tree IS NOT NULL
                  AND IFNULL(tester_id,'')=IFNULL(?, '')
                  AND IFNULL(build_id,'')=IFNULL(?, '')
                ORDER BY created_at ASC
                LIMIT ?
            """, (tester_id, build_id, need))
            rows = c.fetchall() + rows
    X = features_from_rows(rows)
    if X.size == 0: return
    await _train_model_hybrid(X, tester_id, build_id,
                              _get_lock(f"ind:{tester_id}:{build_id}"),
                              desc=f"incremental {tester_id}/{build_id}")

async def _train_general_logic_hybrid(batch_size=1000, min_samples=2):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            SELECT collect_node_tree
            FROM accessibility_data
            WHERE collect_node_tree IS NOT NULL
            ORDER BY created_at DESC
            LIMIT ?
        """, (batch_size,))
        rows = c.fetchall()
        if len(rows) < min_samples:
            need = min_samples - len(rows)
            c.execute("""
                SELECT collect_node_tree
                FROM accessibility_data
                WHERE collect_node_tree IS NOT NULL
                ORDER BY created_at ASC
                LIMIT ?
            """, (need,))
            rows = c.fetchall() + rows
    X = features_from_rows(rows)
    if X.size == 0: return
    await _train_model_hybrid(X, None, None, _get_lock("gen"),
                              max_clusters=5, desc="general")

                              
        
# =========================================================
# A partir de aqu√≠ se mantienen tus endpoints y l√≥gica
# collect, checkForChanges, status, etc.
# Solo hay que reemplazar llamadas a
# _train_incremental_logic y _train_general_logic
# por sus versiones h√≠bridas
# =========================================================

# =========================================================
# UTILIDADES PARA NORMALIZAR CAMPOS

def _insert_diff_trace(tester_id, build_id, screen, message):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO diff_trace (tester_id, build_id, screen_name, message)
            VALUES (?, ?, ?, ?)
        """, (tester_id, build_id, screen, message))
        conn.commit()

def update_diff_trace(changes: List[str]) -> None:
    """
    Actualiza la tabla diff_trace:
      - Si hay cambios, borra mensajes de 'No hay cambios' y agrega cada cambio.
      - Si no hay cambios, asegura que quede un √∫nico registro 'No hay cambios'.
    """
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        if changes:
            c.execute("DELETE FROM diff_trace WHERE message='No hay cambios'")
            for ch in changes:
                c.execute("INSERT INTO diff_trace (message) VALUES (?)", (ch,))
        else:
            c.execute("DELETE FROM diff_trace WHERE message <> 'No hay cambios'")
            c.execute("""
                INSERT INTO diff_trace (message)
                SELECT 'No hay cambios'
                WHERE NOT EXISTS (
                    SELECT 1 FROM diff_trace WHERE message='No hay cambios'
                )
            """)
    conn.commit()
    conn.close()


def last_hash_for_screen(tester_id: Optional[str],
                         screen_name: Optional[str]) -> Optional[str]:
    """
    Devuelve el √∫ltimo screens_id guardado para un tester/pantalla.
    √ötil para saber si la pantalla ya fue procesada.
    """
    conn = sqlite3.connect(DB_NAME)
    try:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT screens_id
            FROM accessibility_data
            WHERE IFNULL(tester_id,'') = IFNULL(?, '')
              AND IFNULL(screen_names,'') = IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT 1
        """, (tester_id or "", screen_name or ""))
        row = cursor.fetchone()
        return row[0] if row else None
    finally:
        conn.close()
        
        
# =========================================================
def _normalize_event_fields(event: AccessibilityEvent) -> dict:
    """
    Devuelve un diccionario normalizado con nombres uniformes y
    alias comunes (snake_case) para su uso en modelos de ML.
    """
    raw = event.dict(by_alias=True, exclude_unset=True)
    tester_id = raw.get("testerId") or event.tester_id
    build_id  = raw.get("buildId")  or event.build_id

    return {
        **raw,
        "tester_id_norm": tester_id.strip() if tester_id else None,
        "build_id_norm":  build_id.strip()  if build_id  else None,
    }          

def normalize_node(node: Dict) -> Dict:
    """Filtra solo las claves estables y convierte None en cadena vac√≠a."""
    return {k: (node.get(k) or "") for k in SAFE_KEYS}

def normalize_tree(nodes: List[Dict]) -> List[Dict]:
    """Normaliza y ordena la lista de nodos para que el orden no afecte el hash."""
    normalized = [normalize_node(n) for n in nodes]
    return sorted(normalized, key=lambda n: (n["className"], n["text"]))

def stable_signature(nodes: List[Dict]) -> str:
    """Genera un hash estable del √°rbol normalizado."""
    norm = normalize_tree(nodes)
    return hashlib.sha256(json.dumps(norm, sort_keys=True).encode()).hexdigest()   
    
# =========================================================
# ENDPOINTS API
# =========================================================
@app.post("/collect")
async def collect_event(event: AccessibilityEvent, background_tasks: BackgroundTasks):
    logger.debug("Raw request: %s", event.model_dump())
    try:
        # Normalizamos variantes de campos que pueden venir con distintos nombres
        raw_nodes = event.collect_node_tree or event.tree_data or []
        normalized_nodes = normalize_tree(raw_nodes)
        signature = stable_signature(raw_nodes)
        norm = _normalize_event_fields(event)
        tester_norm = norm.get("tester_id_norm")
        build_norm = norm.get("build_id_norm")
        screen_name = event.screen_names or ""
        screens_id_val = event.screens_id or norm.get("screensId") or None

        logger.info(f"[collect] normalized tester={tester_norm} build={build_norm} screen={screen_name} screens_id={screens_id_val}")

        # Evitar duplicados inmediatos: comparar √∫ltimo hash
        last = last_hash_for_screen(tester_norm, screen_name)
        logger.debug(f"[collect] last_hash={last} current_hash={screens_id_val}")

        # Si el hash actual es None (no enviado), igual insertamos el snapshot bruto,
        # pero si viene con screens_id y coincide con el √∫ltimo, podemos evitar insertar duplicado.
        do_insert = True
        if screens_id_val and last and str(last) == str(screens_id_val):
            do_insert = False
            logger.info("[collect] Snapshot id√©ntico al √∫ltimo almacenado ‚Äî no se inserta duplicado.")

        if do_insert:
            conn = sqlite3.connect(DB_NAME)
            cursor = conn.cursor()
            logger.info("[collect] Insertando registro en accessibility_data...")
            cursor.execute("""
            INSERT INTO accessibility_data (
                tester_id, build_id, timestamp, event_type, event_type_name,
                package_name, class_name, text, content_description, screens_id,
                screen_names, header_text, actual_device, version,
                collect_node_tree, signature, additional_info, tree_data
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                tester_norm, build_norm, event.timestamp, event.event_type,
                event.event_type_name, event.package_name, event.class_name,
                event.text, event.content_description, screens_id_val,
                event.screen_names, event.header_text, event.actual_device,
                event.version,
                json.dumps(normalized_nodes),   # ‚úÖ √°rbol filtrado
                signature,                      # ‚úÖ hash estable
                json.dumps(event.additional_info) if event.additional_info else None,
                json.dumps(event.tree_data) if event.tree_data else None
            ))
            conn.commit()
            conn.close()
            logger.info("[collect] Insert completado.")
        else:
            logger.info("[collect] Se omiti√≥ insert porque snapshot coincide con √∫ltimo.")

        # A√±adir tarea en background para an√°lisis/entrenamiento (siempre la lanzamos,
        # aunque no se haya insertado para mantener chequeos)
        background_tasks.add_task(analyze_and_train, event)
        return {"status": "success", "inserted": do_insert}
    except Exception as e:
        logger.error(f"Error en /collect: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"status": "error", "message": str(e)})

@app.get("/status")
async def get_status(
    testerId: Optional[str] = Query(None),
    buildId: Optional[str] = Query(None),
    screenName: Optional[str] = Query(None),
    limit: int = Query(5, ge=1, le=100)
):
    def safe_json(txt: str) -> list[Any]:
        try:
            return json.loads(txt) if txt else []
        except Exception:
            return []

    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()

    # query = """
        # SELECT screen_name, removed, added, modified, created_at
        # FROM screen_diffs
    #"""
    query = """
        SELECT s.id,
               s.screen_name,
               s.removed,
               s.added,
               s.modified,
               s.created_at
        FROM screen_diffs AS s
        LEFT JOIN diff_approvals AS a
               ON a.diff_id = s.id     
        WHERE a.id IS NULL 
    """
    clauses, params = [], []



    if testerId:
        clauses.append("s.tester_id = ?")
        params.append(testerId)
    if buildId:
        clauses.append("s.build_id = ?")
        params.append(buildId)
    if screenName:
        clauses.append("s.screen_name = ?")
        params.append(screenName)

    if clauses:
        query += " AND " + " AND ".join(clauses)

    query += " ORDER BY s.created_at DESC LIMIT ?"
    params.append(limit)

    cursor.execute(query, params)

    # if testerId:
        # clauses.append("tester_id = ?")
        # params.append(testerId)
    # if buildId:
        # clauses.append("build_id = ?")
        # params.append(buildId)
    # if screenName:
        # clauses.append("screen_name = ?")
        # params.append(screenName)

    # if clauses:
        # query += " WHERE " + " AND ".join(clauses)

    # query += " ORDER BY created_at DESC LIMIT ?"
    # params.append(limit)

    # cursor.execute(query, params)
    rows = cursor.fetchall()
    conn.close()

    # Filtramos solo diffs con cambios reales
    diffs = []
    for r in rows:
        removed = safe_json(r[1])
        added = safe_json(r[2])
        modified = safe_json(r[3])
        if removed or added or modified:
            diffs.append({
                "screen_name": r[0],
                "removed": removed,
                "added": added,
                "modified": modified,
                "created_at": r[4],
            })

    if not diffs:
        return {}  # O simplemente `return Response(status_code=204)` para no retornar contenido

    return {"status": "changes", "diffs": diffs}


@app.get("/train/general")
async def trigger_general_train(
    batch_size: int = Query(1000, ge=1),  # tama√±o m√°ximo de muestras para entrenar
    min_samples: int = Query(2, ge=1)     # m√≠nimo de muestras para poder entrenar
):
    await _train_general_logic_hybrid(batch_size=batch_size, min_samples=min_samples)
    return {"status": "success", "message": "Entrenamiento general h√≠brido disparado"}


@app.get("/train/incremental")
async def trigger_incremental_train(
    tester_id: str = Query(...),
    build_id: str = Query(...),
    batch_size: int = Query(200, ge=1),
    min_samples: int = Query(2, ge=1)
):
    await _train_incremental_logic_hybrid(
        tester_id=tester_id,
        build_id=build_id,
        batch_size=batch_size,
        min_samples=min_samples
    )
    return {
        "status": "success",
        "message": f"Entrenamiento incremental h√≠brido para {tester_id}/{build_id} disparado"
    }


@app.get("/screen/diffs")
def get_screen_diffs(
    tester_id: Optional[str] = Query(None),
    build_id: Optional[str] = Query(None),
    screen_name: Optional[str] = Query(None)
):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()

    query = """
        SELECT id, tester_id, build_id, screen_name, removed, added, modified, created_at
        FROM screen_diffs
        WHERE 1=1
    """
    params = []
    if tester_id:
        query += " AND IFNULL(tester_id, '') = ?"
        params.append(tester_id)
    if build_id:
        query += " AND IFNULL(build_id, '') = ?"
        params.append(build_id)
    if screen_name:
        query += " AND screen_name = ?"
        params.append(screen_name)

    query += " ORDER BY created_at DESC LIMIT 1"
    cursor.execute(query, tuple(params))
    row = cursor.fetchone()
    conn.close()

    if not row:
        return {"screen_diffs": [], "has_changes": False}

    diff = {
        "id": row[0],
        "tester_id": row[1],
        "build_id": row[2],
        "screen_name": row[3],
        "removed": json.loads(row[4]) if row[4] else [],
        "added": json.loads(row[5]) if row[5] else [],
        "modified": json.loads(row[6]) if row[6] else [],
        "created_at": row[7]
    }
    has_changes = bool(diff["removed"] or diff["added"] or diff["modified"])

    return {"screen_diffs": [diff], "has_changes": has_changes}

@app.get("/screen/exists")
async def screen_exists(buildId: str = Query(...)):
    """
    Devuelve {"exists": true/false} si hay al menos una fila
    en accessibility_data con el build_id indicado.
    """
    conn = sqlite3.connect(DB_NAME)
    logger.info("üîé screen_exists llamado con buildId=%s", buildId)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT 1 FROM accessibility_data
        WHERE TRIM(build_id) = ?    
        LIMIT 1
    """, (buildId,))

    row = cursor.fetchone()
    conn.close()

    logger.info("Resultado de la consulta: %s", bool(row))
    return {"exists": bool(row)}
    
    
@app.post("/approve_diff")
async def approve_diff(request: Request):
    """
    Espera JSON: {"diff_id": <id>} o {"diff_id": "11"}.
    Registra la aprobaci√≥n en DB y devuelve resultado.
    """
    # --- 1. Leer JSON de forma segura ---
    try:
        payload = await request.json()
    except ValueError:
        # El body estaba vac√≠o o no era JSON v√°lido
        raise HTTPException(status_code=400, detail="Cuerpo JSON inv√°lido o vac√≠o")

    # --- 2. Validar diff_id ---
    diff_id = payload.get("diff_id") or payload.get("id")
    if diff_id is None:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id missing"},
        )

    try:
        diff_id_int = int(diff_id)
    except (TypeError, ValueError):
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id must be integer"},
        )

    # --- 3. Guardar en la base de datos ---
    try:
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diff_approvals (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                diff_id INTEGER,
                approved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cursor.execute("INSERT INTO diff_approvals(diff_id) VALUES (?)", (diff_id_int,))
        conn.commit()
    except Exception as db_err:
        logger.exception("Error de base de datos en /approve_diff")
        raise HTTPException(status_code=500, detail=f"DB error: {db_err}")
    finally:
        conn.close()

    logger.info("Diff %s aprobado v√≠a API", diff_id_int)
    return {"status": "success", "diff_id": diff_id_int}


@app.post("/reject_diff")
async def reject_diff(request: Request):
    """
    Espera JSON: {"diff_id": <id>} o {"diff_id": "11"}.
    Registra el rechazo en DB y devuelve resultado.
    """
    # --- 1. Leer JSON de forma segura ---
    try:
        payload = await request.json()
    except ValueError:
        raise HTTPException(status_code=400, detail="Cuerpo JSON inv√°lido o vac√≠o")

    # --- 2. Validar diff_id ---
    diff_id = payload.get("diff_id") or payload.get("id")
    if diff_id is None:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id missing"},
        )

    try:
        diff_id_int = int(diff_id)
    except (TypeError, ValueError):
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id must be integer"},
        )

    # --- 3. Guardar en la base de datos ---
    try:
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diff_rejections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                diff_id INTEGER,
                rejected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cursor.execute("INSERT INTO diff_rejections(diff_id) VALUES (?)", (diff_id_int,))
        conn.commit()
    except Exception as db_err:
        logger.exception("Error de base de datos en /reject_diff")
        raise HTTPException(status_code=500, detail=f"DB error: {db_err}")
    finally:
        conn.close()

    logger.info("Diff %s rechazado v√≠a API", diff_id_int)
    return {"status": "success", "diff_id": diff_id_int}

@app.post("/cleanup_diffs")
async def cleanup_diffs(older_than_days: int = 90):
    """
    Borra diffs aprobados o rechazados anteriores a `older_than_days`.
    """
    from datetime import datetime, timedelta
    import sqlite3

    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()

    # Borrar diffs aprobados
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))

    # Borrar diffs rechazados
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_rejections r ON r.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))

    conn.commit()
    conn.close()
    return {"status": "success", "message": f"Difs anteriores a {cutoff_str} eliminados."}
    
@app.post("/cleanup_approvals_rejections")
async def cleanup_approvals_rejections(older_than_days: int = 90):
    from datetime import datetime, timedelta
    import sqlite3

    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()

    c.execute("DELETE FROM diff_approvals WHERE created_at < ?", (cutoff_str,))
    c.execute("DELETE FROM diff_rejections WHERE created_at < ?", (cutoff_str,))

    conn.commit()
    conn.close()
    return {"status": "success", "message": f"Approvals y rejections anteriores a {cutoff_str} eliminados."}
    


@app.on_event("startup")
@repeat_every(seconds=86400)  # 1 d√≠a
def scheduled_cleanup() -> None:
    from datetime import datetime, timedelta
    import sqlite3

    older_than_days = 90
    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_rejections r ON r.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))
    c.execute("DELETE FROM diff_approvals WHERE created_at < ?", (cutoff_str,))
    c.execute("DELETE FROM diff_rejections WHERE created_at < ?", (cutoff_str,))
    conn.commit()
    conn.close()
    
    
   
@router.get("/reports/screen-changes")
def screen_changes(build_id: str, db=Depends(get_db)):
    rows = db.execute("""
        SELECT screen_name, removed, added, modified, created_at
        FROM screen_diffs
        WHERE build_id = ?
        ORDER BY created_at DESC
    """, (build_id,)).fetchall()

    return [
        {
            "screen_name": r["screen_name"],
            "removed": json.loads(r["removed"] or "[]"),
            "added": json.loads(r["added"] or "[]"),
            "modified": json.loads(r["modified"] or "[]"),
            "timestamp": r["created_at"]
        }
        for r in rows
    ]
    
@router.get("/reports/ui-stability")
def ui_stability(
    start_date: datetime,
    end_date: datetime,
    db=Depends(get_db)
):
    rows = db.execute("""
        SELECT screen_name,
               COUNT(*) as changes,
               COUNT(DISTINCT build_id) as builds_affected
        FROM screen_diffs
        WHERE created_at BETWEEN ? AND ?
        GROUP BY screen_name
        ORDER BY changes DESC
    """, (start_date, end_date)).fetchall()

    return [
        {
            "screen_name": r["screen_name"],
            "total_changes": r["changes"],
            "builds_affected": r["builds_affected"]
        }
        for r in rows
    ]   
    
@router.get("/reports/capture-coverage")
def capture_coverage(
    base_build: str,
    compare_build: str,
    db=Depends(get_db)
):
    base = db.execute("""
        SELECT DISTINCT screen_names
        FROM accessibility_data
        WHERE build_id = ?
    """, (base_build,)).fetchall()

    comp = db.execute("""
        SELECT DISTINCT screen_names
        FROM accessibility_data
        WHERE build_id = ?
    """, (compare_build,)).fetchall()

    base_screens = {r[0] for r in base}
    comp_screens = {r[0] for r in comp}

    return {
        "base_build": base_build,
        "compare_build": compare_build,
        "only_in_base": list(base_screens - comp_screens),
        "only_in_compare": list(comp_screens - base_screens),
        "common": list(base_screens & comp_screens)
    }    
    
app.include_router(router)