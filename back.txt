from fastapi import FastAPI, Query, BackgroundTasks, Request, APIRouter, HTTPException, Depends
from typing import Optional, Union, List, Dict, Any
from pydantic import BaseModel, Field
import sqlite3, json, joblib, numpy as np, os, hashlib, logging, asyncio
from sklearn.cluster import MiniBatchKMeans
from hmmlearn import hmm
from fastapi.responses import JSONResponse
from fastapi_utils.tasks import repeat_every
from diff_model.predict_diff import router as diff_router
from datetime import datetime

# =========================================================
# CONFIGURACIÓN
# =========================================================
app = FastAPI()
router = APIRouter() 
DB_NAME = "accessibility.db"
MODELS_DIR = "models"
logger = logging.getLogger("myapp")
logger.setLevel(logging.INFO)
app.include_router(diff_router)



if not logger.hasHandlers():
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(ch)

os.makedirs(MODELS_DIR, exist_ok=True)
TRAIN_GENERAL_ON_COLLECT = True

# Locks para entrenamientos concurrentes
_model_locks: Dict[str, asyncio.Lock] = {}


# === Reentrenamiento automático del modelo diff ===
@app.on_event("startup")
@repeat_every(seconds=3600)  # cada hora, ajusta el intervalo a lo que necesites
def retrain_model() -> None:
    """
    Reentrena el modelo diff con las últimas aprobaciones/rechazos
    sin necesidad de parar el servidor.
    """
    from diff_model.train_diff_model import train_and_save
    train_and_save()


def _get_lock(key: str) -> asyncio.Lock:
    if key not in _model_locks:
        _model_locks[key] = asyncio.Lock()
    return _model_locks[key]

# =========================================================
# BASE DE DATOS
# =========================================================

def get_db():
    """
    Devuelve una conexión SQLite que se cierra automáticamente
    al finalizar la petición.
    """
    db = sqlite3.connect(DB_NAME)
    db.row_factory = sqlite3.Row  # Para poder acceder a columnas por nombre
    try:
        yield db
    finally:
        db.close()
        
        
def init_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS accessibility_data (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            timestamp INTEGER,
            event_type TEXT,
            event_type_name TEXT,
            package_name TEXT,
            class_name TEXT,
            text TEXT,
            content_description TEXT,
            screens_id TEXT,
            screen_names TEXT,
            header_text TEXT,
            actual_device TEXT,
            signature TEXT,
            version TEXT,
            collect_node_tree TEXT,
            additional_info TEXT,
            tree_data TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS screen_diffs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            screen_name TEXT,
            removed TEXT,
            added TEXT,
            modified TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_trace (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            tester_id TEXT,
            build_id TEXT,
            screen_name TEXT,
            message TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_approvals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            diff_id INTEGER,
            approved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS diff_rejections (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            diff_id INTEGER,
            rejected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    c.execute("""
        CREATE INDEX IF NOT EXISTS idx_data_tester_build_screen
        ON accessibility_data(tester_id, build_id, screen_names, created_at DESC)
    """)
    c.execute("""
        CREATE INDEX IF NOT EXISTS idx_diffs_tester_build_screen
        ON screen_diffs(tester_id, build_id, screen_name, created_at DESC)
    """)
    conn.commit()
    conn.close()
    
init_db()

# =========================================================
# MODELOS DE ENTRADA
# =========================================================
class AccessibilityEvent(BaseModel):
    # Identificación de tester y build (usa alias en camelCase para que coincida con el JSON entrante)
    tester_id: Optional[str] = Field(None, alias="actualDevice")
    build_id: Optional[str] = Field(None, alias="version")
    
    # Datos básicos del evento de accesibilidad
    timestamp: Optional[int] = Field(None, alias="timestamp")
    event_type: Optional[int] = Field(None, alias="eventType")
    event_type_name: Optional[str] = Field(None, alias="eventTypeName")
    package_name: Optional[str] = Field(None, alias="packageName")
    class_name: Optional[str] = Field(None, alias="className")
    text: Optional[str] = Field(None, alias="text")
    content_description: Optional[str] = Field(None, alias="contentDescription")
    
    # Información de flujo/pantalla
    screens_id: Optional[str] = Field(None, alias="screensId")
    screen_names: Optional[str] = Field(None, alias="screenNames")
    header_text: Optional[str] = Field(None, alias="headerText")
    
    # Datos de dispositivo y versión de la app
    actual_device: Optional[str] = Field(None, alias="actualDevice")    
    version: Optional[str] = Field(None, alias="version")
    
    # Árbol de nodos capturado (puede ser dict o lista de nodos)
    # collect_node_tree: Optional[Union[Dict, List]] = Field(None, alias="collectNodeTree")
    collect_node_tree: Optional[Union[Dict[str, Any], List[Any]]] = Field(
        None, alias="collectNodeTree"
    )
    
    # Datos adicionales para enriquecer el modelo (libres)
    additional_info: Optional[Dict[str, Any]] = Field(None, alias="additionalInfo")
    tree_data: Optional[Dict[str, Any]] = Field(None, alias="treeData")

    class Config:
    # Permite poblar el modelo con los nombres de campo internos o los alias del JSON
        allow_population_by_field_name = True
    
    # Acepta campos extra que la app pueda enviar en el futuro sin romper validación
        extra = "allow"

# =========================================================
# UTILIDADES PARA ÁRBOLES Y HASH ESTABLE
# =========================================================
SAFE_KEYS = ["className", "text", "desc", "viewId", "pkg"]


def ui_structure_features(tree: dict) -> list[float]:
    """
    Cuenta componentes y propiedades de accesibilidad.
    Devuelve:
    [buttons, text_fields, menus, recycler_views, web_views,
     enabled_count, clickable_count, focusable_count]
    """
    counts = {
        "buttons": 0,
        "text_fields": 0,
        "menus": 0,
        "recycler_views": 0,
        "web_views": 0
    }
    props = {
        "enabled": 0,
        "clickable": 0,
        "focusable": 0
    }

    def traverse(node):
        if not isinstance(node, dict):
            return
        cls = node.get("className", "")
        if "Button" in cls:        counts["buttons"]        += 1
        if "EditText" in cls:      counts["text_fields"]    += 1
        if "Menu" in cls:          counts["menus"]          += 1
        if "RecyclerView" in cls:  counts["recycler_views"] += 1
        if "WebView" in cls:       counts["web_views"]      += 1

        if node.get("enabled"):    props["enabled"]   += 1
        if node.get("clickable"):  props["clickable"] += 1
        if node.get("focusable"):  props["focusable"] += 1

        for ch in node.get("children", []):
            traverse(ch)

    traverse(tree)
    return list(counts.values()) + list(props.values())

def input_features(events):
    total_chars   = sum(len(e.text or "") for e in events if e.type=="input")
    upper_ratio   = total_chars and sum(ch.isupper() for e in events if e.type=="input" for ch in e.text)/total_chars
    action_seq    = [e.type for e in events]   # p.ej. ["tap","scroll","tap"]
    seq_entropy   = sequence_entropy(action_seq)
    return [total_chars, upper_ratio, seq_entropy]

def ensure_list(tree):
    if isinstance(tree, str):
        try:
            return json.loads(tree)
        except Exception:
            return []
    return tree or []

def normalize_node(node: Dict) -> Dict:
    return {k: (node.get(k) or "") for k in SAFE_KEYS}

def normalize_tree(nodes: List[Dict]) -> List[Dict]:
    return sorted([normalize_node(n) for n in nodes if isinstance(n, dict)],
                  key=lambda n: (n["className"], n["text"]))

def stable_signature(nodes: List[Dict]) -> str:
    return hashlib.sha256(json.dumps(normalize_tree(nodes), sort_keys=True).encode()).hexdigest()


def compare_trees(old_tree, new_tree):
    old_tree = ensure_list(old_tree)
    new_tree = ensure_list(new_tree)
    def make_key(n):
        if not isinstance(n, dict): return None
        parts = [n.get("viewId"), n.get("className"), n.get("headerText")]
        return "|".join([str(p) for p in parts if p])
    old_idx = {make_key(n): n for n in old_tree if make_key(n)}
    new_idx = {make_key(n): n for n in new_tree if make_key(n)}
    removed = [n for k, n in old_idx.items() if k not in new_idx]
    added   = [n for k, n in new_idx.items() if k not in old_idx]
    modified = []
    for k, nn in new_idx.items():
        if k in old_idx:
            changes = {f: {"old": old_idx[k].get(f), "new": nn.get(f)}
                       for f in ["text","contentDescription","checked","enabled"]
                       if old_idx[k].get(f) != nn.get(f)}
            if changes:
                modified.append({"node": {"key": k}, "changes": changes})
    return removed, added, modified

# =========================================================
# VECTORIZACIÓN Y FEATURES
# =========================================================
def compare_trees(old_tree, new_tree):
    old_tree = ensure_list(old_tree)
    new_tree = ensure_list(new_tree)
    def make_key(n):
        if not isinstance(n, dict): return None
        parts = [n.get("viewId"), n.get("className"), n.get("headerText")]
        return "|".join([str(p) for p in parts if p])
    old_idx = {make_key(n): n for n in old_tree if make_key(n)}
    new_idx = {make_key(n): n for n in new_tree if make_key(n)}
    removed = [n for k, n in old_idx.items() if k not in new_idx]
    added   = [n for k, n in new_idx.items() if k not in old_idx]
    modified = []
    for k, nn in new_idx.items():
        if k in old_idx:
            changes = {f: {"old": old_idx[k].get(f), "new": nn.get(f)}
                       for f in ["text","contentDescription","checked","enabled"]
                       if old_idx[k].get(f) != nn.get(f)}
            if changes:
                modified.append({"node": {"key": k}, "changes": changes})
    return removed, added, modified

def features_from_rows(rows) -> np.ndarray:
    vecs = [vector_from_tree(r[0]) for r in rows if r and r[0]]
    return np.vstack(vecs) if vecs else np.empty((0,3))
    
    
# =========================================================
# VECTORIZACIÓN Y FEATURES
# =========================================================
def vector_from_tree(tree_str: str) -> np.ndarray:
    """
    Genera un vector:
    [total_nodes, max_depth, text_nodes,
     buttons, text_fields, menus, recycler_views, web_views,
     enabled_count, clickable_count, focusable_count]
    """
    try:
        tree = json.loads(tree_str)
    except Exception:
        # Vector con 11 ceros si el JSON no es válido
        return np.zeros(11, dtype=float)

    def walk(node, depth=1):
        if not isinstance(node, dict):
            return (0, depth, 0)
        children = node.get("children", [])
        total, max_d, txt = 1, depth, 1 if node.get("text") else 0
        for ch in children:
            t, d, c = walk(ch, depth + 1)
            total += t
            max_d = max(max_d, d)
            txt += c
        return total, max_d, txt

    if isinstance(tree, list):
        totals = [walk(n) for n in tree]
        total_nodes = sum(t[0] for t in totals)
        max_depth   = max((t[1] for t in totals), default=0)
        text_nodes  = sum(t[2] for t in totals)
        # Para ui_structure_features, empaquetamos en un dict "raíz"
        struct_vec  = ui_structure_features({"children": tree})
    else:
        total_nodes, max_depth, text_nodes = walk(tree)
        struct_vec  = ui_structure_features(tree)

    base_vec = [total_nodes, max_depth, text_nodes]
    return np.array(base_vec + struct_vec, dtype=float)

def features_from_rows(rows) -> np.ndarray:
    vectors = [vector_from_tree(r[0]) for r in rows if r and r[0]]
    return np.vstack(vectors) if vectors else np.empty((0, 11))    

# =========================================================
# ENTRENAMIENTO HÍBRIDO (KMeans + HMM)
# =========================================================
# async def _train_model_hybrid(X, tester_id, build_id, lock: asyncio.Lock,
                              # max_clusters=3, min_samples=2,
                              # desc="", n_hmm_states=5):
    # async with lock:
        # if len(X) < min_samples:
            # logger.info(f"[train_hybrid] muestras insuficientes {desc}")
            # return
        # kmeans = MiniBatchKMeans(n_clusters=min(max_clusters,len(X)), random_state=42).fit(X)
        # hmm_model = hmm.GaussianHMM(
            # n_components=min(n_hmm_states,len(X)),
            # covariance_type="diag", n_iter=100
        # ).fit(X, [len(X)])
        # base = os.path.join(MODELS_DIR, tester_id or "general", build_id or "default")
        # os.makedirs(base, exist_ok=True)
        # joblib.dump({"kmeans": kmeans, "hmm": hmm_model}, os.path.join(base, "model.pkl"))
        # logger.info(f"[train_hybrid] Modelo guardado {desc}")
        
        
async def _train_model_hybrid(
    X,
    tester_id,
    build_id,
    lock: asyncio.Lock,
    max_clusters=2,        # ↓ menos clusters en KMeans
    min_samples=2,
    desc="",
    n_hmm_states=2         # ↓ menos estados ocultos en el HMM
):
    async with lock:
        if len(X) < min_samples:
            logger.info(f"[train_hybrid] muestras insuficientes {desc}")
            return

        # KMeans con máximo 2 clusters (o tantos como muestras haya)
        kmeans = MiniBatchKMeans(
            n_clusters=min(max_clusters, len(X)),
            random_state=42
        ).fit(X)

        # HMM con máximo 2 estados ocultos
        hmm_model = hmm.GaussianHMM(
            n_components=min(n_hmm_states, len(X)),
            covariance_type="diag",
            n_iter=100    # puedes bajar a 50 si quieres aún más rápido
        ).fit(X, [len(X)])

        base = os.path.join(MODELS_DIR, tester_id or "general", build_id or "default")
        os.makedirs(base, exist_ok=True)
        joblib.dump({"kmeans": kmeans, "hmm": hmm_model}, os.path.join(base, "model.pkl"))
        logger.info(f"[train_hybrid] Modelo guardado {desc}")

# =========================================================
# ENTRENAMIENTO HÍBRIDO (KMeans + HMM)  – versión mejorada
# =========================================================


async def analyze_and_train(event: AccessibilityEvent):
    norm = _normalize_event_fields(event)
    t_id, b_id = norm.get("tester_id_norm"), norm.get("build_id_norm")
    s_name = event.screen_names or ""
    latest_tree = ensure_list(event.collect_node_tree or event.tree_data or [])
    sig = stable_signature(latest_tree)

    with sqlite3.connect(DB_NAME) as conn:
        if conn.execute("""
            SELECT 1 FROM accessibility_data
            WHERE IFNULL(screen_names,'')=IFNULL(?, '')
              AND IFNULL(tester_id,'')=IFNULL(?, '')
              AND signature=?
            LIMIT 1
        """, (s_name, t_id, sig)).fetchone():
            logger.info("[diff-trace] firma ya existente, skip")
            return

    with sqlite3.connect(DB_NAME) as conn:
        prev = conn.execute("""
            SELECT collect_node_tree
            FROM accessibility_data
            WHERE IFNULL(screen_names,'')=IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT 3
        """, (s_name,)).fetchall()

    prev_trees = [ensure_list(r[0]) for r in prev]
    if not prev_trees:
        _insert_diff_trace(t_id, b_id, s_name, "Sin cambios (primera captura)")
        await _train_incremental_logic_hybrid(t_id, b_id)
        if TRAIN_GENERAL_ON_COLLECT:
            await _train_general_logic_hybrid()
        return

    removed, added, modified = [], [], []
    for p in prev_trees:
        r,a,m = compare_trees(p, latest_tree)
        removed += r; added += a; modified += m

    if not (removed or added or modified):
        _insert_diff_trace(t_id, b_id, s_name, "Sin cambios significativos")
        return

    removed_j, added_j, mod_j = map(lambda x: json.dumps(x, sort_keys=True),
                                    (removed, added, modified))
    with sqlite3.connect(DB_NAME) as conn:
        if conn.execute("""
            SELECT 1 FROM screen_diffs
            WHERE IFNULL(screen_name,'')=IFNULL(?, '')
              AND removed=? AND added=? AND modified=?
            LIMIT 1
        """, (s_name, removed_j, added_j, mod_j)).fetchone():
            return
        if conn.execute("""
            SELECT 1
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.screen_name=? AND s.removed=? AND s.added=? AND s.modified=?
            LIMIT 1
        """, (s_name, removed_j, added_j, mod_j)).fetchone():
            return
        conn.execute("""
            INSERT INTO screen_diffs (tester_id, build_id, screen_name, removed, added, modified)
            VALUES (?,?,?,?,?,?)
        """, (t_id, b_id, s_name, removed_j, added_j, mod_j))
        conn.commit()

    _insert_diff_trace(t_id, b_id, s_name,
                       f"Removed={len(removed)}, Added={len(added)}, Modified={len(modified)}")

    await _train_incremental_logic_hybrid(t_id, b_id)
    if TRAIN_GENERAL_ON_COLLECT:
        await _train_general_logic_hybrid()


# =========================================================
# ENTRENAMIENTO INCREMENTAL (versión original conservada)
# =========================================================
async def _train_incremental_logic_hybrid(tester_id: str, build_id: str,
                                          batch_size=200, min_samples=2):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            SELECT DISTINCT collect_node_tree
            FROM accessibility_data
            WHERE collect_node_tree IS NOT NULL
              AND IFNULL(tester_id,'')=IFNULL(?, '')
              AND IFNULL(build_id,'')=IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT ?
        """, (tester_id, build_id, batch_size))
        rows = c.fetchall()
        if len(rows) < min_samples:
            need = min_samples - len(rows)
            c.execute("""
                SELECT DISTINCT collect_node_tree
                FROM accessibility_data
                WHERE collect_node_tree IS NOT NULL
                  AND IFNULL(tester_id,'')=IFNULL(?, '')
                  AND IFNULL(build_id,'')=IFNULL(?, '')
                ORDER BY created_at ASC
                LIMIT ?
            """, (tester_id, build_id, need))
            rows = c.fetchall() + rows
    X = features_from_rows(rows)
    if X.size == 0: return
    await _train_model_hybrid(X, tester_id, build_id,
                              _get_lock(f"ind:{tester_id}:{build_id}"),
                              desc=f"incremental {tester_id}/{build_id}")

async def _train_general_logic_hybrid(batch_size=1000, min_samples=2):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            SELECT collect_node_tree
            FROM accessibility_data
            WHERE collect_node_tree IS NOT NULL
            ORDER BY created_at DESC
            LIMIT ?
        """, (batch_size,))
        rows = c.fetchall()
        if len(rows) < min_samples:
            need = min_samples - len(rows)
            c.execute("""
                SELECT collect_node_tree
                FROM accessibility_data
                WHERE collect_node_tree IS NOT NULL
                ORDER BY created_at ASC
                LIMIT ?
            """, (need,))
            rows = c.fetchall() + rows
    X = features_from_rows(rows)
    if X.size == 0: return
    await _train_model_hybrid(X, None, None, _get_lock("gen"),
                              max_clusters=5, desc="general")

                              
        
# =========================================================
# A partir de aquí se mantienen tus endpoints y lógica
# collect, checkForChanges, status, etc.
# Solo hay que reemplazar llamadas a
# _train_incremental_logic y _train_general_logic
# por sus versiones híbridas
# =========================================================

# =========================================================
# UTILIDADES PARA NORMALIZAR CAMPOS

def _insert_diff_trace(tester_id, build_id, screen, message):
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO diff_trace (tester_id, build_id, screen_name, message)
            VALUES (?, ?, ?, ?)
        """, (tester_id, build_id, screen, message))
        conn.commit()

def update_diff_trace(changes: List[str]) -> None:
    """
    Actualiza la tabla diff_trace:
      - Si hay cambios, borra mensajes de 'No hay cambios' y agrega cada cambio.
      - Si no hay cambios, asegura que quede un único registro 'No hay cambios'.
    """
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        if changes:
            c.execute("DELETE FROM diff_trace WHERE message='No hay cambios'")
            for ch in changes:
                c.execute("INSERT INTO diff_trace (message) VALUES (?)", (ch,))
        else:
            c.execute("DELETE FROM diff_trace WHERE message <> 'No hay cambios'")
            c.execute("""
                INSERT INTO diff_trace (message)
                SELECT 'No hay cambios'
                WHERE NOT EXISTS (
                    SELECT 1 FROM diff_trace WHERE message='No hay cambios'
                )
            """)
    conn.commit()
    conn.close()


def last_hash_for_screen(tester_id: Optional[str],
                         screen_name: Optional[str]) -> Optional[str]:
    """
    Devuelve el último screens_id guardado para un tester/pantalla.
    Útil para saber si la pantalla ya fue procesada.
    """
    conn = sqlite3.connect(DB_NAME)
    try:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT screens_id
            FROM accessibility_data
            WHERE IFNULL(tester_id,'') = IFNULL(?, '')
              AND IFNULL(screen_names,'') = IFNULL(?, '')
            ORDER BY created_at DESC
            LIMIT 1
        """, (tester_id or "", screen_name or ""))
        row = cursor.fetchone()
        return row[0] if row else None
    finally:
        conn.close()
        
        
# =========================================================
def _normalize_event_fields(event: AccessibilityEvent) -> dict:
    """
    Devuelve un diccionario normalizado con nombres uniformes y
    alias comunes (snake_case) para su uso en modelos de ML.
    """
    raw = event.dict(by_alias=True, exclude_unset=True)
    tester_id = raw.get("testerId") or event.tester_id
    build_id  = raw.get("buildId")  or event.build_id

    return {
        **raw,
        "tester_id_norm": tester_id.strip() if tester_id else None,
        "build_id_norm":  build_id.strip()  if build_id  else None,
    }          

def normalize_node(node: Dict) -> Dict:
    """Filtra solo las claves estables y convierte None en cadena vacía."""
    return {k: (node.get(k) or "") for k in SAFE_KEYS}

def normalize_tree(nodes: List[Dict]) -> List[Dict]:
    """Normaliza y ordena la lista de nodos para que el orden no afecte el hash."""
    normalized = [normalize_node(n) for n in nodes]
    return sorted(normalized, key=lambda n: (n["className"], n["text"]))

def stable_signature(nodes: List[Dict]) -> str:
    """Genera un hash estable del árbol normalizado."""
    norm = normalize_tree(nodes)
    return hashlib.sha256(json.dumps(norm, sort_keys=True).encode()).hexdigest()   
    
# =========================================================
# ENDPOINTS API
# =========================================================
@app.post("/collect")
async def collect_event(event: AccessibilityEvent, background_tasks: BackgroundTasks):
    logger.debug("Raw request: %s", event.model_dump())
    try:
        # Normalizamos variantes de campos que pueden venir con distintos nombres
        raw_nodes = event.collect_node_tree or event.tree_data or []
        normalized_nodes = normalize_tree(raw_nodes)
        signature = stable_signature(raw_nodes)
        norm = _normalize_event_fields(event)
        tester_norm = norm.get("tester_id_norm")
        build_norm = norm.get("build_id_norm")
        screen_name = event.screen_names or ""
        screens_id_val = event.screens_id or norm.get("screensId") or None

        logger.info(f"[collect] normalized tester={tester_norm} build={build_norm} screen={screen_name} screens_id={screens_id_val}")

        # Evitar duplicados inmediatos: comparar último hash
        last = last_hash_for_screen(tester_norm, screen_name)
        logger.debug(f"[collect] last_hash={last} current_hash={screens_id_val}")

        # Si el hash actual es None (no enviado), igual insertamos el snapshot bruto,
        # pero si viene con screens_id y coincide con el último, podemos evitar insertar duplicado.
        do_insert = True
        if screens_id_val and last and str(last) == str(screens_id_val):
            do_insert = False
            logger.info("[collect] Snapshot idéntico al último almacenado — no se inserta duplicado.")

        if do_insert:
            conn = sqlite3.connect(DB_NAME)
            cursor = conn.cursor()
            logger.info("[collect] Insertando registro en accessibility_data...")
            cursor.execute("""
            INSERT INTO accessibility_data (
                tester_id, build_id, timestamp, event_type, event_type_name,
                package_name, class_name, text, content_description, screens_id,
                screen_names, header_text, actual_device, version,
                collect_node_tree, signature, additional_info, tree_data
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                tester_norm, build_norm, event.timestamp, event.event_type,
                event.event_type_name, event.package_name, event.class_name,
                event.text, event.content_description, screens_id_val,
                event.screen_names, event.header_text, event.actual_device,
                event.version,
                json.dumps(normalized_nodes),   # ✅ árbol filtrado
                signature,                      # ✅ hash estable
                json.dumps(event.additional_info) if event.additional_info else None,
                json.dumps(event.tree_data) if event.tree_data else None
            ))
            conn.commit()
            conn.close()
            logger.info("[collect] Insert completado.")
        else:
            logger.info("[collect] Se omitió insert porque snapshot coincide con último.")

        # Añadir tarea en background para análisis/entrenamiento (siempre la lanzamos,
        # aunque no se haya insertado para mantener chequeos)
        background_tasks.add_task(analyze_and_train, event)
        return {"status": "success", "inserted": do_insert}
    except Exception as e:
        logger.error(f"Error en /collect: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"status": "error", "message": str(e)})

@app.get("/status")
async def get_status(
    testerId: Optional[str] = Query(None),
    buildId: Optional[str] = Query(None),
    screenName: Optional[str] = Query(None),
    limit: int = Query(5, ge=1, le=100)
):
    def safe_json(txt: str) -> list[Any]:
        try:
            return json.loads(txt) if txt else []
        except Exception:
            return []

    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()

    # query = """
        # SELECT screen_name, removed, added, modified, created_at
        # FROM screen_diffs
    #"""
    query = """
        SELECT s.id,
               s.screen_name,
               s.removed,
               s.added,
               s.modified,
               s.created_at
        FROM screen_diffs AS s
        LEFT JOIN diff_approvals AS a
               ON a.diff_id = s.id     
        WHERE a.id IS NULL 
    """
    clauses, params = [], []



    if testerId:
        clauses.append("s.tester_id = ?")
        params.append(testerId)
    if buildId:
        clauses.append("s.build_id = ?")
        params.append(buildId)
    if screenName:
        clauses.append("s.screen_name = ?")
        params.append(screenName)

    if clauses:
        query += " AND " + " AND ".join(clauses)

    query += " ORDER BY s.created_at DESC LIMIT ?"
    params.append(limit)

    cursor.execute(query, params)

    # if testerId:
        # clauses.append("tester_id = ?")
        # params.append(testerId)
    # if buildId:
        # clauses.append("build_id = ?")
        # params.append(buildId)
    # if screenName:
        # clauses.append("screen_name = ?")
        # params.append(screenName)

    # if clauses:
        # query += " WHERE " + " AND ".join(clauses)

    # query += " ORDER BY created_at DESC LIMIT ?"
    # params.append(limit)

    # cursor.execute(query, params)
    rows = cursor.fetchall()
    conn.close()

    # Filtramos solo diffs con cambios reales
    diffs = []
    for r in rows:
        removed = safe_json(r[1])
        added = safe_json(r[2])
        modified = safe_json(r[3])
        if removed or added or modified:
            diffs.append({
                "screen_name": r[0],
                "removed": removed,
                "added": added,
                "modified": modified,
                "created_at": r[4],
            })

    if not diffs:
        return {}  # O simplemente `return Response(status_code=204)` para no retornar contenido

    return {"status": "changes", "diffs": diffs}


@app.get("/train/general")
async def trigger_general_train(
    batch_size: int = Query(1000, ge=1),  # tamaño máximo de muestras para entrenar
    min_samples: int = Query(2, ge=1)     # mínimo de muestras para poder entrenar
):
    await _train_general_logic_hybrid(batch_size=batch_size, min_samples=min_samples)
    return {"status": "success", "message": "Entrenamiento general híbrido disparado"}


@app.get("/train/incremental")
async def trigger_incremental_train(
    tester_id: str = Query(...),
    build_id: str = Query(...),
    batch_size: int = Query(200, ge=1),
    min_samples: int = Query(2, ge=1)
):
    await _train_incremental_logic_hybrid(
        tester_id=tester_id,
        build_id=build_id,
        batch_size=batch_size,
        min_samples=min_samples
    )
    return {
        "status": "success",
        "message": f"Entrenamiento incremental híbrido para {tester_id}/{build_id} disparado"
    }


@app.get("/screen/diffs")
def get_screen_diffs(
    tester_id: Optional[str] = Query(None),
    build_id: Optional[str] = Query(None),
    screen_name: Optional[str] = Query(None)
):
    conn = sqlite3.connect(DB_NAME)
    cursor = conn.cursor()

    query = """
        SELECT id, tester_id, build_id, screen_name, removed, added, modified, created_at
        FROM screen_diffs
        WHERE 1=1
    """
    params = []
    if tester_id:
        query += " AND IFNULL(tester_id, '') = ?"
        params.append(tester_id)
    if build_id:
        query += " AND IFNULL(build_id, '') = ?"
        params.append(build_id)
    if screen_name:
        query += " AND screen_name = ?"
        params.append(screen_name)

    query += " ORDER BY created_at DESC LIMIT 1"
    cursor.execute(query, tuple(params))
    row = cursor.fetchone()
    conn.close()

    if not row:
        return {"screen_diffs": [], "has_changes": False}

    diff = {
        "id": row[0],
        "tester_id": row[1],
        "build_id": row[2],
        "screen_name": row[3],
        "removed": json.loads(row[4]) if row[4] else [],
        "added": json.loads(row[5]) if row[5] else [],
        "modified": json.loads(row[6]) if row[6] else [],
        "created_at": row[7]
    }
    has_changes = bool(diff["removed"] or diff["added"] or diff["modified"])

    return {"screen_diffs": [diff], "has_changes": has_changes}

@app.get("/screen/exists")
async def screen_exists(buildId: str = Query(...)):
    """
    Devuelve {"exists": true/false} si hay al menos una fila
    en accessibility_data con el build_id indicado.
    """
    conn = sqlite3.connect(DB_NAME)
    logger.info("🔎 screen_exists llamado con buildId=%s", buildId)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT 1 FROM accessibility_data
        WHERE TRIM(build_id) = ?    
        LIMIT 1
    """, (buildId,))

    row = cursor.fetchone()
    conn.close()

    logger.info("Resultado de la consulta: %s", bool(row))
    return {"exists": bool(row)}
    
    
@app.post("/approve_diff")
async def approve_diff(request: Request):
    """
    Espera JSON: {"diff_id": <id>} o {"diff_id": "11"}.
    Registra la aprobación en DB y devuelve resultado.
    """
    # --- 1. Leer JSON de forma segura ---
    try:
        payload = await request.json()
    except ValueError:
        # El body estaba vacío o no era JSON válido
        raise HTTPException(status_code=400, detail="Cuerpo JSON inválido o vacío")

    # --- 2. Validar diff_id ---
    diff_id = payload.get("diff_id") or payload.get("id")
    if diff_id is None:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id missing"},
        )

    try:
        diff_id_int = int(diff_id)
    except (TypeError, ValueError):
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id must be integer"},
        )

    # --- 3. Guardar en la base de datos ---
    try:
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diff_approvals (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                diff_id INTEGER,
                approved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cursor.execute("INSERT INTO diff_approvals(diff_id) VALUES (?)", (diff_id_int,))
        conn.commit()
    except Exception as db_err:
        logger.exception("Error de base de datos en /approve_diff")
        raise HTTPException(status_code=500, detail=f"DB error: {db_err}")
    finally:
        conn.close()

    logger.info("Diff %s aprobado vía API", diff_id_int)
    return {"status": "success", "diff_id": diff_id_int}


@app.post("/reject_diff")
async def reject_diff(request: Request):
    """
    Espera JSON: {"diff_id": <id>} o {"diff_id": "11"}.
    Registra el rechazo en DB y devuelve resultado.
    """
    # --- 1. Leer JSON de forma segura ---
    try:
        payload = await request.json()
    except ValueError:
        raise HTTPException(status_code=400, detail="Cuerpo JSON inválido o vacío")

    # --- 2. Validar diff_id ---
    diff_id = payload.get("diff_id") or payload.get("id")
    if diff_id is None:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id missing"},
        )

    try:
        diff_id_int = int(diff_id)
    except (TypeError, ValueError):
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "diff_id must be integer"},
        )

    # --- 3. Guardar en la base de datos ---
    try:
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diff_rejections (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                diff_id INTEGER,
                rejected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        cursor.execute("INSERT INTO diff_rejections(diff_id) VALUES (?)", (diff_id_int,))
        conn.commit()
    except Exception as db_err:
        logger.exception("Error de base de datos en /reject_diff")
        raise HTTPException(status_code=500, detail=f"DB error: {db_err}")
    finally:
        conn.close()

    logger.info("Diff %s rechazado vía API", diff_id_int)
    return {"status": "success", "diff_id": diff_id_int}

@app.post("/cleanup_diffs")
async def cleanup_diffs(older_than_days: int = 90):
    """
    Borra diffs aprobados o rechazados anteriores a `older_than_days`.
    """
    from datetime import datetime, timedelta
    import sqlite3

    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()

    # Borrar diffs aprobados
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))

    # Borrar diffs rechazados
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_rejections r ON r.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))

    conn.commit()
    conn.close()
    return {"status": "success", "message": f"Difs anteriores a {cutoff_str} eliminados."}
    
@app.post("/cleanup_approvals_rejections")
async def cleanup_approvals_rejections(older_than_days: int = 90):
    from datetime import datetime, timedelta
    import sqlite3

    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()

    c.execute("DELETE FROM diff_approvals WHERE created_at < ?", (cutoff_str,))
    c.execute("DELETE FROM diff_rejections WHERE created_at < ?", (cutoff_str,))

    conn.commit()
    conn.close()
    return {"status": "success", "message": f"Approvals y rejections anteriores a {cutoff_str} eliminados."}
    


@app.on_event("startup")
@repeat_every(seconds=86400)  # 1 día
def scheduled_cleanup() -> None:
    from datetime import datetime, timedelta
    import sqlite3

    older_than_days = 90
    cutoff = datetime.utcnow() - timedelta(days=older_than_days)
    cutoff_str = cutoff.strftime("%Y-%m-%d %H:%M:%S")

    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_approvals a ON a.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))
    c.execute("""
        DELETE FROM screen_diffs
        WHERE id IN (
            SELECT s.id
            FROM screen_diffs s
            JOIN diff_rejections r ON r.diff_id = s.id
            WHERE s.created_at < ?
        )
    """, (cutoff_str,))
    c.execute("DELETE FROM diff_approvals WHERE created_at < ?", (cutoff_str,))
    c.execute("DELETE FROM diff_rejections WHERE created_at < ?", (cutoff_str,))
    conn.commit()
    conn.close()
    
    
   
@router.get("/reports/screen-changes")
def screen_changes(build_id: str, db=Depends(get_db)):
    rows = db.execute("""
        SELECT screen_name, removed, added, modified, created_at
        FROM screen_diffs
        WHERE build_id = ?
        ORDER BY created_at DESC
    """, (build_id,)).fetchall()

    return [
        {
            "screen_name": r["screen_name"],
            "removed": json.loads(r["removed"] or "[]"),
            "added": json.loads(r["added"] or "[]"),
            "modified": json.loads(r["modified"] or "[]"),
            "timestamp": r["created_at"]
        }
        for r in rows
    ]
    
@router.get("/reports/ui-stability")
def ui_stability(
    start_date: datetime,
    end_date: datetime,
    db=Depends(get_db)
):
    rows = db.execute("""
        SELECT screen_name,
               COUNT(*) as changes,
               COUNT(DISTINCT build_id) as builds_affected
        FROM screen_diffs
        WHERE created_at BETWEEN ? AND ?
        GROUP BY screen_name
        ORDER BY changes DESC
    """, (start_date, end_date)).fetchall()

    return [
        {
            "screen_name": r["screen_name"],
            "total_changes": r["changes"],
            "builds_affected": r["builds_affected"]
        }
        for r in rows
    ]   
    
@router.get("/reports/capture-coverage")
def capture_coverage(
    base_build: str,
    compare_build: str,
    db=Depends(get_db)
):
    base = db.execute("""
        SELECT DISTINCT screen_names
        FROM accessibility_data
        WHERE build_id = ?
    """, (base_build,)).fetchall()

    comp = db.execute("""
        SELECT DISTINCT screen_names
        FROM accessibility_data
        WHERE build_id = ?
    """, (compare_build,)).fetchall()

    base_screens = {r[0] for r in base}
    comp_screens = {r[0] for r in comp}

    return {
        "base_build": base_build,
        "compare_build": compare_build,
        "only_in_base": list(base_screens - comp_screens),
        "only_in_compare": list(comp_screens - base_screens),
        "common": list(base_screens & comp_screens)
    }    
    
app.include_router(router)